"use strict";(globalThis.webpackChunkwriteup=globalThis.webpackChunkwriteup||[]).push([[8749],{952:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>i,metadata:()=>a,toc:()=>h});var a=s(2880),o=s(4848),n=s(8453);const i={title:"Natas Level 3",authors:["shubham"],tags:["natas","overthewire","cybersecurity"]},r=void 0,l={authorsImageUrls:[void 0]},h=[{value:"Natas 3",id:"natas-3",level:2},{value:"Theory",id:"theory",level:3},{value:"Solution",id:"solution",level:3}];function c(e){const t={a:"a",code:"code",h2:"h2",h3:"h3",p:"p",pre:"pre",...(0,n.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.h2,{id:"natas-3",children:"Natas 3"}),"\n",(0,o.jsx)(t.h3,{id:"theory",children:"Theory"}),"\n",(0,o.jsx)(t.p,{children:"Today\u2019s internet is indexed by search-engine crawlers, such that Google and Co know what content exists on websites to improve search engine results. The robots.txt file exists on servers to tell these crawlers and other web bots, which part of the website can be visited. It allows defining a user-agent aka for what specific bot the rules should be, and which page of the website the user-agent is not allowed to visit."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-bash",children:"User-agent: Googlebot\nDisallow: /nogooglebot/\n\nUser-agent: *\nAllow: /\n\nSitemap: https://www.example.com/sitemap.xml\n"})}),"\n",(0,o.jsxs)(t.p,{children:["Here, for all crawlers/bots the whole website can be visited, except Google\u2019s crawler, which cannot visit the \u2019nogooglebot\u2019 directory. Also noticeable is the sitemap link. A sitemap is another file to give web crawlers information about the website.\nIt is important to be aware that the \u2018robots.txt\u2019 file does NOT serve a security purpose. The disallowed pages can still be visited and might still show up in search engines. Read more about this file in the (Google Developer Docs)[",(0,o.jsx)(t.a,{href:"https://developers.google.com/search/docs/crawling-indexing/robots/create-robots-txt",children:"https://developers.google.com/search/docs/crawling-indexing/robots/create-robots-txt"}),"]."]}),"\n",(0,o.jsx)(t.h3,{id:"solution",children:"Solution"}),"\n",(0,o.jsx)(t.p,{children:"Opening the website and looking at the source code, we can find a comment, which gives us a hint: ."}),"\n",(0,o.jsxs)(t.p,{children:["This sound like the robots.txt file, I explained in the theory section. Navigating to the site: ",(0,o.jsx)(t.a,{href:"http://natas3.natas.labs.overthewire.org/robots.txt",children:"http://natas3.natas.labs.overthewire.org/robots.txt"})," reveals that the file exists and it shows a disallowed path. Navigating to this path, again leads to a public directory with a \u2018user.txt\u2019 file, which contains the password to the next level."]})]})}function u(e={}){const{wrapper:t}={...(0,n.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},2880:e=>{e.exports=JSON.parse('{"permalink":"/writeup/blog/2024/10/18/natas-3","source":"@site/blog/2024-10-18-natas-3.md","title":"Natas Level 3","description":"Natas 3","date":"2024-10-18T00:00:00.000Z","tags":[{"inline":true,"label":"natas","permalink":"/writeup/blog/tags/natas"},{"inline":true,"label":"overthewire","permalink":"/writeup/blog/tags/overthewire"},{"inline":true,"label":"cybersecurity","permalink":"/writeup/blog/tags/cybersecurity"}],"hasTruncateMarker":false,"authors":[{"name":"Shubham","title":"Cybersecurity Enthusiast","url":"https://github.com/Shubham-0-0-7","imageURL":"https://github.com/Shubham-0-0-7.png","key":"shubham","page":null}],"frontMatter":{"title":"Natas Level 3","authors":["shubham"],"tags":["natas","overthewire","cybersecurity"]},"unlisted":false,"prevItem":{"title":"Natas Level 2","permalink":"/writeup/blog/2024/10/18/natas-2"},"nextItem":{"title":"Natas Level 4","permalink":"/writeup/blog/2024/10/18/natas-4"}}')},8453:(e,t,s)=>{s.d(t,{R:()=>i,x:()=>r});var a=s(6540);const o={},n=a.createContext(o);function i(e){const t=a.useContext(n);return a.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),a.createElement(n.Provider,{value:t},e.children)}}}]);